{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of Multiarm Bandit Algorythms\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mab.montecarlo import test_alorythm\n",
    "from mab.bernoulliarm import BernoulliArm\n",
    "from mab.epsilongreedy import EpsilonGreedy\n",
    "from mab.randomselect import RandomSelect\n",
    "from mab.annealingsoftmax import AnnealingSoftmax\n",
    "from mab.metric import Metric\n",
    "from mab.ab import AB\n",
    "from mab.ucb import UCB\n",
    "from mab.softmax import Softmax\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(results, metric=\"accuracy\", title=\"\"):\n",
    "    \"\"\" Help function to make plotly charts of the metric results\n",
    "        results - dictionaries with results of experiments\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    for res in results:\n",
    "        fig.add_trace(go.Scatter(x=np.array(range(horizon)),\n",
    "                                 y=np.array(res[metric]),\n",
    "                                 mode='lines', name=res['algorythm']))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "arms = list(map(lambda x: BernoulliArm(x), mu))\n",
    "n_arms = len(mu)\n",
    "horizon = 250\n",
    "num_sims = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorythms = [EpsilonGreedy(0.1, n_arms=n_arms),\n",
    "              EpsilonGreedy(0.5, n_arms=n_arms),\n",
    "              EpsilonGreedy(0.9, n_arms=n_arms),\n",
    "              Softmax(0.1, n_arms=n_arms),\n",
    "              Softmax(0.5, n_arms=n_arms),\n",
    "              RandomSelect(n_arms=n_arms),\n",
    "              AnnealingSoftmax(n_arms=n_arms),\n",
    "              UCB(n_arms=n_arms),\n",
    "              AB(n_arms=n_arms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for algo in algorythms:\n",
    "    res = test_alorythm(algo, arms, num_sims, horizon)\n",
    "\n",
    "    res['algorythm'] = algo.name\n",
    "    res['accuracy'] = Metric.accuracy(4,\n",
    "                                      res['times'],\n",
    "                                      res['chosen_arms'],\n",
    "                                      num_sims)\n",
    "\n",
    "    res['average_reward'] = Metric.average_reward(res['times'],\n",
    "                                                  res['rewards'],\n",
    "                                                  num_sims)\n",
    "\n",
    "    res['cumulative_reward'] = Metric.cumulative_reward(res['times'],\n",
    "                                                        res['cumulative_rewards'],\n",
    "                                                        num_sims)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(results[:-1], metric = \"accuracy\", title=\"Accuracy\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(results[:-1], metric = \"average_reward\", title=\"Average Reward\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(results, metric = \"cumulative_rewards\", title=\"Cumulative Rewards\")\n",
    "fig.add_trace(go.Scatter(x=np.array(range(horizon)),\n",
    "                         y=np.array(range(horizon)),\n",
    "                         mode='lines', name=\"Best Possible\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
